Writing the entire code for a VAE is beyond the scope of a single response, as it involves several components and depends on the specific machine learning framework you are using (for example, TensorFlow, PyTorch). However, we can provide a simplified example using TensorFlow in Python. 
Note: This is a basic illustration; you may need to adapt it based on your specific requirements.
import tensorflow as tf # loading libraries
from tensorflow.keras import layers
from tensorflow.keras.models import Model

# Define the VAE architecture
class VAE(tf.keras.Model):
    def __init__(self, latent_dim):
        super(VAE, self).__init__()
        self.latent_dim = latent_dim

        # Encoder
        self.encoder = tf.keras.Sequential([
            layers.InputLayer(input_shape=(28, 28, 1)),
            layers.Conv2D(64, (3, 3), activation='relu', strides=(2, 2), padding='same'),
            layers.Flatten(),
            layers.Dense(latent_dim + latent_dim),
        ])

        # Decoder
        self.decoder = tf.keras.Sequential([
            layers.InputLayer(input_shape=(latent_dim,)),
            layers.Dense(units=7*7*32, activation='relu'),
            layers.Reshape(target_shape=(7, 7, 32)),
            layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=(2, 2), padding='same'),
            layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same'),
        ])

    def sample(self, eps=None):
        if eps is None:
            eps = tf.random.normal(shape=(100, self.latent_dim))
        return self.decode(eps, apply_sigmoid=True)

    def encode(self, x):
        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
        return mean, logvar

    def reparameterize(self, mean, logvar):
        eps = tf.random.normal(shape=mean.shape)
        return eps * tf.exp(logvar * 0.5) + mean

    def decode(self, z, apply_sigmoid=False):
        logits = self.decoder(z)
        if apply_sigmoid:
            probs = tf.sigmoid(logits)
            return probs
        return logits

# Instantiate the VAE model
latent_dim = 2
vae = VAE(latent_dim)

# Define the loss function
def compute_loss(model, x):
    mean, logvar = model.encode(x)
    z = model.reparameterize(mean, logvar)
    x_logit = model.decode(z)

    # Compute reconstruction loss
    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)
    reconstruction_loss = tf.reduce_sum(cross_entropy, axis=(1, 2, 3))

    # Compute KL divergence (regularization term)
    kl_divergence = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar), axis=1)

    # Total loss
    return tf.reduce_mean(reconstruction_loss + kl_divergence)

# Define the optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)

# Training step
@tf.function
def train_step(model, x, optimizer):
    with tf.GradientTape() as tape:
        loss = compute_loss(model, x)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

# Example usage
(x_train, _), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255.0

num_epochs = 10
for epoch in range(num_epochs):
    for step, x_batch in enumerate(x_train):
        x_batch = tf.expand_dims(x_batch, axis=0)
        loss = train_step(vae, x_batch, optimizer)

    print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}')

