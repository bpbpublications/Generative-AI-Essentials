TThe application of RL in a self-driving car scenario, offering insights into the code structure and expected output.

```python
import random

# Define the state space and action space
state_space = ["on_track", "off_track", "obstacle", "destination_reached"]
action_space = ["move_forward", "turn_left", "turn_right"]

# Define the rewards
rewards = {
    "on_track": {
        "move_forward": 1,
        "turn_left": -0.2,
        "turn_right": -0.2,
    },
    "off_track": {
        "move_forward": -1,
        "turn_left": -1,
        "turn_right": -1,
    },
    "obstacle": {
        "move_forward": -2,
        "turn_left": -0.5,
        "turn_right": -0.5,
    },
    "destination_reached": {
        "move_forward": 10,
        "turn_left": 0,
        "turn_right": 0,
    },
}

# Define the RL agent
class RLAgent:
    def __init__(self):
        self.state = "on_track"

    def choose_action(self):
        return random.choice(action_space)

    def receive_reward(self, state, action):
        return rewards[state][action]

# Simulation loop
agent = RLAgent()
for _ in range(10):
    action = agent.choose_action()
    reward = agent.receive_reward(agent.state, action)
    print(f"Agent takes action: {action}, receives reward: {reward}")
    if agent.state == "destination_reached":
        break
    new_state = random.choice(state_space)
    agent.state = new_state
    print(f"New state: {new_state}")

